---
title: "Lithic artifacts and texture analysis: a low magnification approach"
author: "Guillermo Bustos-Pérez$^{1,2,3}"
output: 
  md_document:
    variant: gfm
  pdf_document: default
  bookdown::html_document2: 
    toc: true
    toc_float: true
    toc_depth: 4
    fig_caption: yes
    number_sections: FALSE
  bookdown::word_document2:
    reference_docx: "References_Styles/word-styles-reference.docx"
    fig_caption: yes
    number_sections: FALSE
    code_folding: "hide"
bibliography: "References_Styles/References.bib"
csl: "References_Styles/journal-of-archaeological-science.csl"
link-citations: true
linkcolor: blue
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

$^1$Universidad Autónoma de Madrid. Departamento de Prehistoria y Arqueología, Campus de Cantoblanco, 28049 Madrid, Spain.   
$^2$Institut Català de Paleoecologia Humana i Evolució Social (IPHES), Zona Educacional 4, Campus Sescelades URV (Edifici W3), 43007 Tarragona, Spain.      
$^3$Àrea de Prehistoria, Universitat Rovira i Virgili (URV), Avinguda de Catalunya 35, 43002 Tarragona, Spain.    

\ 
   
<div align="justify">    
   
**Abstract**   

Lithic artifacts are some of the most common and numerous remains from paleolithic archaeological sites. However, their deposition into the archaeological record can be the result of multiple and distant episodes of discard which can undergo a series of post depositional alterations. Because the numerous amounts of lithic remains, a quick, flexible, and effective method for identifying degrees of alteration in the surface of lithic implements is highly desirable. This would favor the analysis of complete lithic assemblages, inferring their overall integrity, identify different episodes of alteration, and even identify post-depositional alterations of different nature (such as rounding, dragging or wind erosion). This study examines the use gray scale level images to obtain quantitative measures from the surface of flint artifacts and determine if they capture changes due to post-depositional alterations. An experimental collection of flints was subjected to sequential episodes of rounding in a tumbling machine. After each episode, photographs with a Dino-Lite Edge 3.0 AM73915MZT USB microscope were taken, allowing to obtain quantitative values of surface using gray scale level values. The surface quantitative values were employed as variables in machine learning models to determine degree of exposure time and the most important variables for discrimination. Results indicate that the extraction of metrics from gray scale level images successfully capture changes in the surface of flint artifacts caused by post-depositional alterations. Additional results provide insights into which areas to sample in search for post-depositional alterations, and underline the importance of particle size causing the damage.      

**Key words**: lithic taphonomy; experimental archaeology; machine learning    

\ 

## **1. Introduction**   
\ 
Lithic artifacts constitute one of the most common remains from paleolithic archaeological sites. When analyzing lithic assemblages, a key factor to consider is determining the degree of post-depositional alterations undergone by the assemblage, since eco-cultural inferences are drawn from them. Methods for determining assemblage integrity usually focus on the assemblage as a hole. These methods often include spatial analysis of artifacts and analysis of fabrics to determine if water flow has resulted in a reorganization of the spatial distribution and orientations [@lenoble_fabric_2004; @mcpherron_artifact_2005; @mcpherron_additional_2018; @petraglia_water_1994; @schick_experimentally-derived_1987]. Also, lithic size distribution analysis are realized to determine if post-depositional processes have resulted in sorting of the materials [@bertran_particle_2012; @maillo_fernandez_proporciones_1998]. Another option is to focus directly on individual lithic artifacts to determine the degree of alteration that they have undergone [@chu_micro-abrasion_2015; @levi_sala_use_1986]. Although being more time consuming, this type of analysis provides a higher resolution, allowing to establish a graduation in the degree of post-depositional alterations, combine individual data of stone tools with spatial analysis, possible dissection of archaeological episodes, or to detect several episodes of recycling. Individual determination of degree of alteration undergone by a lithic artifact uses microscopic analysis to measure ridge width, visually analyze the surface to determine the existence and intensity of abrasion, and examine the edges to determine the presence of detachments coming from particle impact or dulling of the edges in more extreme cases [@bustos-perez_experimental_2019; @chambers_like_2003; @shackley_stream_1974].   

Post-depositional alterations can affect and interact with stone tools in two broad ways which will be recorded by the tool surface. First, the type of fluvial sedimentary processes which can affect lithic artifacts. Fluvial sedimentary processes are characterized by particle transport and these processes are not homogeneous. Lithic artifacts can form part of these process as another particle, or remain static and be affected by particles being transported. Commonly, three modes of particle transport are described in fluvial sedimentary contexts: rolling, sliding and saltation [@alhusban_assessing_2021], although the last one is not a common form of alteration in the case of stone tools [@petraglia_water_1994]. Additional to alterations coming from fluvial contexts, stone artifacts might also be affected by aeolian particle transport, usually resulting in wind abrasion [@stapert_natural_1976]. The second factor of variability affecting post-depositional alterations of stone tools is the degree of exposure and speed on which a stone tool enters the archaeological record [@petraglia_water_1994; @schick_experimentally-derived_1987; @schiffer_archaeological_1972]. For example, it is expected that partially buried artifacts with water and sediment flowing above them will present modifications in the exposed surface, while the edges and the burred surface will remain semi-intact [@petraglia_water_1994]. A different scenario happens when artifacts are transported by rolling in coarse sediments, resulting in abrasion of all surfaces, dulling of the edges, but at the same time, the impact from coarse particles might result in freshly detached surfaces and edges which undergo new modifications until the artifact enters the archaeological record [@petraglia_water_1994; @sieveking_transport_1987]. Finally, a same homogeneous lithic assemblage might be affected differentially by stream abrasion. This differential alteration is consequence of the complex structure of water streams, were the overall slope of the terrain, energy and charge of the flow, morphology of the channel (which also affects the lateral deposition of sediments) and several other factors result in complex and uneven structures [@rust_structure_1972; @jain_where_2008; @montgomery_channel-reach_1997].    

Thus, obtaining quantitative data of surface modifications can complement values of ridge width, and help identify and interpret type of particle transport undergone by, or affecting a stone artifact. Ideally, quantitative values of surface abrasion would be extracted from each stone artifact, allowing to maximize information and interpretation of the formation process of an archaeological site. However, as previously mentioned, stone artifacts are some of the most common remains from Paleolithic archaeological sites. Thus, a versatile, fast and light time-consuming method is highly desirable since it would enable to analyze large quantities of lithic materials.    

Use-wear studies have shown surface quantification can discriminate worked materials and offer a high resolution analysis of the activities carried out [@stemp_quantification_2009; @ibanez_quantitative_2021; @ibanez_identifying_2019; @ibanez_cereal_2016; @bostrom_quantifying_2022; @pedergnana_polish_2020; @evans_laser_2008]. Previous approaches in the study of use-wear have used grayscale level values for surface analysis of different worked materials [@adan_spatial_2003; @barcelo_image_2001; @bietti_image_1996; @pijoan-lopez_variabilidad_2002; @vila_caracterizacion_1993; @grace_quantification_1985]. Gray scale level analysis offers a series of advantages such as being fast to perform, require low equipment investment and can be applied in the field. This research presents results from a sequential experimentation in which a set of experimentally knapped flakes have undergone rounding process in a tumbler. After each episode of rounding, the same areas are photographed, and quantitative measures of flake surface using grayscale level values are employed to characterize the surface. Following the extraction of quantitative values, machine learning models are employed to determine the degree of separation between rounding times and most important features.    

## **2. Methods**    

### **2.1 Experimental sample and setting**   

Two blocks of flint representing two different types (Type I and Type II) were experimentally knapped. The two types correspond to the South Madrid Miocene Flint [@bustillo_caracterizacion_2012; @bustillo_caracteristicas_2005] although from two different locations. South Madrid Miocene flints were formed by the replacement of sedimentary rocks which had filled the original basin. This replacement of the sedimentary rocks is considered to have taken place under continental conditions such as alluvial plain deposits, shallow lacustrine waters, and marshes [@bustillo_caracterizacion_2012]. Macroscopic analysis of the flints shows that they present a fine opaque homogeneous surface, with colors being blue/grey and reddish/ocher. There is also a relative absence of opal, although geodes and pseudo-morphs can be observed. From these two blocks three flakes were selected from Type 1, and four flakes were selected from Type 2.     
!["Experimental sample of flint flakes (photographs by M. D. Guillén)."](Figures/01-Presenting-materials.png)

In order to simulate sedimentary abrasion, the flakes were introduced by pairs into a tumbler machine (KT-3010 SUPER-TUMBLER) along with a mix of sand and water (30/40% of water with a total weight of 5 kg). Sediment was obtained from the quaternary levels of the Madrid basin and it is composed of fine sands with silt and partial carbonation.    

All flakes were submitted to three cumulative cycles of tumbling with times set to 1h, 5h and 10h. Prior to their introduction into the tumbling machine, 6 photographs (3 per each side) of the surface of each flake were taken in order to have references of texture metrics from flint flakes. Previous research [@chu_micro-abrasion_2015; @bustos-perez_experimental_2019] suggests that, for the development of post depositional alterations, a decrease in particle size increases the heterogeneity in the speed, intensity and location of their development. Thus, after the first cycle of rounding (1h), each flint flake was screened using the Dino-Lite Edge 3.0 AM73915MZT USB microscope in order to determine and photograph areas which had developed sedimentary abrasion. These areas were photographed in the subsequent rounding cycles, allowing to obtain sequential images and data of surface change for cumulative times of 1h, 5h and 10h. The dataset of images was supplemented by the inclusion of sample photographs of macroscopically recognizable geological neocortex of flints from the same formation, providing a total of 269 photographs.    

```{r}
# Load and inspect the data
load("Data/Sequential Data v1.01.RData")
kableExtra::kable(head(Sequential.Data))
```
```{r, message=FALSE, warning=FALSE}
#load packages
library(tidyverse); library(caret); library(pROC)
```


```{r}
# Counts of number of images per time of exposure
# (sequential images taken after the first episode of abrasion are not included)
kableExtra::kable(Sequential.Data %>% 
  filter(Photo.Type != "Sequential" | Flake.Time ==  "Fresh" ) %>% 
  group_by(Flake.Time) %>% 
  summarise(
    N.by.Flakes = n()))
```


### **2.2 Cleaning protocol, image acquisition and processing**   

Workflow developed in the present study includes a series of steps prior to extracting quantitative data from the images. These steps are: cleaning protocol to remove contaminants from the stone tool surface, image capture, and image enhancement.   

Multiple works emphasize the need of cleaning protocols to remove modern contaminants prior to analysis [@pedergnana_modern_2016; @asryan_results_2020; @olle_use_2014; @fernandez-marchena_microscopic_2016]. A multi-step procedure based on previous studies was adopted [@pedergnana_modern_2016] in order to retrieve possible contaminants. This multi-step procedure included a sonic bath in 2% neutral soap (Derquim) solution during 10 to 15 minutes, followed by a second sonic bath in pure acetone during another 10 to 15 minutes. After each step the lithic artefacts were introduced in a water bath and finally dried using pressure air. During the cleaning protocol and microscopic analysis all artefacts were manipulated using surgical gloves.     

!["Effects of manual manipulation without protection on flint surface. Left: surface photographed after the application of cleaning protocol and manipulation of the artefact using protection. Right: same surface after manual manipulation without protection"](Figures/02-Effects-of-grease.png)

All surface photographs were taken using a Dino-Lite Edge 3.0 AM73915MZT USB microscope at 120 magnifications with a field of view (FOV) of 3.28 x 2.46 mm and a pixel ratio of 2548 x 1918. The USB microscope was mounted in a Dino-Lite RK-06-AE stand in order to ensure verticality, and a N3C-D2 diffuser cap was used to ensure homogeneous distribution of light. During the realization of each photograph, the region of interest of the flint was manually positioned as horizontal as possible [@calandra_workflow_2022]. To avoid problems due to focus variation, each surface was photographed several times at different heights, and the obtained sequences were mounted using Helicon Focus 7.7.2.   

A common problem of images obtained from USB microscopes is the lack of detail due to saturation in one of the grey level values and the effects of different lightning or surface color. This saturation is often observed as a general glaze in one of the grey level values and results in a low-quality image with poor detail. To increase detail and quality prior to the analysis, all images were subjected to a two-step process. First, the Fiji [@schindelin_fiji_2012] plugging “Subtract background” was employed to minimize effects of different lightning and changes in flint color. Second, the function “Enhance contrast” was employed to desaturate the images by normalizing their histograms. This process provides a gray-scale level image employed as input for the statistical analysis.   

!["Two examples of image enhancement. Left: original images taken with the AM73915MZT USB microscope and using a N3C-D2 diffuser cap. Center: images after retrieving the background to avoid effects of different lightning or color. Right: after normalizing the histogram to increase detail and avoid saturation. Top row: geological neocortex. Bottom row: fresh surface of an experimentally knapped flake."](Figures/03-Retrieve-background-and-normalize.jpg)

### **2.3 Statistical analysis**   

The present work uses three sets of statistical metrics to analyze obtained images. The first set of statistical metrics correspond to descriptive statistics (mean, standard deviation, mode, median skewness, and kurtosis). The second set of statistical measures corresponds to measures of roughness. Surface parameters using the “R” prefix use profiles as input. The present work uses a Fiji/ImageJ plugging were R-values are obtained on the hole surface following the ISO 4287/2000 standard [@chinga_roughness_2002; @chinga_quantification_2007]. These measures are:   

  * **Root mean square deviation/roughness (Rq):** indicator of surface roughness.   
  * **Arithmetical mean deviation (Ra):** which indicates the deviation of a surface from a mean height.   
  * **Skewness of the assessed profile (Rsk):** indicator of the departure from surface symmetry. Negative values indicate a surface made of deep valleys, and positive values indicates peaks and asperities.      
  * **Kurtosis of the assessed profile (Rku):** which indicates the sharpness of the peaks. Low values indicate blunt peaks, while high values indicate sharp peaks.    
  
Analysis of intensity values through the Gray Level Co-occurrence Matrix (GLCM; [@haralick_textural_1973]) takes into consideration the spatial distribution of intensity values. The GLCM works in two steps [@haralick_textural_1973]. First, using a given distance and direction it builds a matrix which captures the relationship of intensity between pairs of pixels (reference and neighbor). Second, for every x and y it considers the co-occurrence of values, forming a new matrix. From this new matrix, a series of statistical descriptors are derived [@haralick_textural_1973].   

  * **Angular Second Moment (ASM)** is measure of homogeneity in the image. Homogeneous images (with low gray-tone transitions) will have fewer entries of large magnitude. Thus, homogeneous images will have high ASM values, while the opposite will be true for non-homogeneous images.    
  * **Contrast (CONT)** is a value of the amount of local variations. High values are indicating a lot of local variation and low values indicate few local variations.   
  * **Correlation (CORR)** which measures gray-tone linear-dependencies in the image. It indicates how a reference pixel is related to its neighbor. A 0 value indicates it is uncorrelated, and 1 indicates a perfect correlation.   
  * **Inverse Different Moment (IDM)** also referred as homogeneity. It obtains the measures of the closeness of the distribution of the GLCM elements to the GLCM diagonal.   
  * **Entropy (ENT)** is a measure of the amount of irremediable chaos or disorder in an image. High values of entropy indicate values of similar magnitude, while low values indicate unequal entries.     
  
As previously mentioned, use of the GLCM requires selecting pixel distance between reference and neighbor, and direction on which to establish the distance [@haralick_textural_1973]. For this, it is common to test for different combinations of distances and directions since images at different magnifications, different field of view and different resolution might require variation in pixel distance and directions [@bietti_image_1996; @grace_quantification_1985]. For the present study, a preliminary test indicated that using four distances at 5, 10, 15 and 20 pixels in the four possible directions (north, east, south and west) to set the GLCM presented the best results for discrimination.    

Calculation of all metrics was done using the free software Fiji [@schindelin_fiji_2012]. Roughness metrics calculation was implemented through the “Roughness calculation” plugin [@chinga_roughness_2002]. GLCM and texture metrics were calculated using the “GLCM Texture” plugging [@cabrera_texture_2006].     

### **2.4 Machine Learning models and evaluation**     

Data from descriptive statistics, roughness and texture were employed as variable for the training of Machine Learning models in order to predict the time of exposure to sedimentary abrasion. The ten models tested were:   

  * **Linear discriminant analysis (LDA):** reduces dimensionality for the purpose of maximizing the separation between classes while decision boundaries divide the predictor range into regions [@fisher_use_1936; @james_introduction_2013].     
  * **K-nearest neighbor (KNN):** classifies cases by assigning the class of similar known cases. The ‘k’ in KNN refers to the number of cases (neighbors) to consider when assigning a class, which must be found by testing different values. Given that KNN uses distance metrics to compute nearest neighbors and that each variable is in different scales, the data must be scaled and centered prior to fitting the model [@cover_nearest_1967; @lantz_machine_2019].   
  * **Logistic regression:** essentially adapts continuous regression predictions to categorical outcomes [@cramer_early_2004; @walker_estimation_1967].  
  * **Decision tree with C5.0 algorithm:** an improvement on decision trees for classification [@quinlan_improved_1996; @quinlan_c4_2014].    
  * **Random forest:** made of decision trees. Each tree is grown from a random sample of the data and variables, allowing for each tree to grow differently and to better reflect the complexity of the data [@breiman_random_2001].    
  * **Generalized boosted model [@greenwell_package_2019; @ridgeway_generalized_2007]:** implements the gradient boosted machine [@friedman_greedy_2001; @friedman_stochastic_2002] making it possible to detect learning deficiencies and increase model accuracy for a set of random forests.    
  * **Supported vector machines (SVM):** fit hyperplanes into a multidimensional space with the objective of creating homogeneous partitions [@cortes_support-vector_1995; @frey_letter_1991]. The present study tests SVM with linear, radial and polynomial kernels.   
  * **Naïve Bayes classifier:** computes class probabilities using Bayes’ rule [@weihs_klar_2005].   

![Example of the "one versus all" approach in order to obtain ROC curves and AUC values in the case of multiclass problems](Figures/04-Thresholds-multiple-categories.png){width=600}

All models are evaluated using 10×100 k-fold cross validation (10 folds and 100 cycles), providing measures of accuracy. Using a 10-fold division, each fold will have 43 data points. Each fold serves subsequently as test set for a trained model. Although computationally more expensive, this guarantees that all data points will serve as test sets.  The 100 cycles provide a random shuffling of the dataset prior to fold division, thus ensuring that the composition of the folds varies in each cycle and it does not play a significant role in the evaluation of the models.   

Machine Learning models commonly use a 0.5 classification threshold to assign categories. However, classification thresholds can be modified to balance the ability of model to detect true positives and avoid false positives which are respectively referred as sensitivity and specificity. The receiver operating characteristic (ROC) curve is employed to systematically evaluate the ratio of detected true positives while avoiding false positives [@bradley_use_1997; @spackman_signal_1989]. The ROC curve allows visually analyzing model performance and calculating the AUC, which ranges from 1 (perfect classifier) to 0.5 (random classifier). The AUC is a measure of performance derived from the receiver operating characteristic (ROC) curve. The ROC curve is used to evaluate the ratio of detected true positives while avoiding false positives [@bradley_use_1997; @spackman_signal_1989]. The ROC curve makes it possible to visually analyze model performance and calculate the AUC, which ranges from 1 (perfect classifier) to 0.5 (random classifier). The ROC and AUC are commonly applied in two-class problems and their extension to multiclass problems is usually done through pairwise analysis.   

In the case of multiclass problems, the AUC provides two groups of values: first, each class obtains an AUC value using a “one vs all” approach; second, a general AUC value of model performance is obtained from the average of each class AUC [@hand_simple_200; @robin_proc_2011]. In the case of the ROC curve, individual curves of each class are plotted using the previously mentioned “one vs all” approach. The present study tested 10 different models with a three-class classification problem which would involve a total of 30 different ROC curves (three curves per 10 models). In this paper, we have provided only the three ROC curves of the best model. When analyzing lithic materials, the use of thresholds to guarantee true positives and avoid false positives is of special interest. The use of thresholds better indicates the accuracy of a model considering these probability values. In the present study we have slightly variated the interpretation of the AUC values [@lantz_machine_2019] with intervals interpreted as follows:    

  * 0.9 to 1: outstanding   
  * 0.85 to 0.9: excellent    
  * 0.8 to 0.85: good    
  * 0.75 to 0.8: acceptable    
  * 0.7 to 0.75: fair   
  * 0.6 to 0.7: poor    
  * 0.5 to 0.6: no discrimination     
  
Strong levels of correlation are present between the variables of the present study. The issue of multicollinearity in classification remains a matter of debate. It is commonly pointed that for multiple linear regressions, collinearity affects the interpretation of coefficients (variables), but does not affect the quality of the predictions [@james_introduction_2013; @alin_multicollinearity_2010; @chan_mitigating_2022]. Additional arguments indicate that if the collinearity between variables of the training set is also present in the test set, it should not be considered a problem. In the present study variables presenting perfect levels of collinearity (mean and Ra) are excluded from the training of machine learning models, and feature importance is explored among non-correlated features. After evaluating and determining the best model, an additional model on PCA values was trained in order to determine possible overfitting.    

```{r, message=FALSE, warning=FALSE}

# Check for collinearity of the data
r <- cor(Sequential.Data[,2:16], 
         use = "complete.obs")^2

ggcorrplot::ggcorrplot(r, 
           hc.order = TRUE, 
           type = "lower",
           lab_size = 2,
           tl.cex = 8,
           lab = TRUE) +
  ggsci::scale_fill_gsea(reverse = FALSE) +
  theme(legend.position = "none",
        axis.text = element_text(color = "black"))
```

### **2.5 Training of Machine Learning models**     

The following code was set to evaluate and train the machine learning models described in the methods section. All resultant models are available in the *Data* section.    


```{r, eval=FALSE}
#### Train the models ####
# Validation
library(caret)
trControl <- trainControl(method  = "repeatedcv",
                          verboseIter = TRUE,
                          number  = 10,
                          repeats = 100,
                          savePredictions = "final",
                          classProbs = TRUE)

Data <- Sequential.Data %>% select(-c(Mean, Ra))

frmla <- as.formula(
  paste("Flake.Time", paste(colnames(Data[,2:14]), collapse = " + "), sep = " ~ "))

# LDA model
set.seed(123)
LDA.model <- train(frmla, 
                   Data,
                   method = "lda",
                   preProc = c("center", "scale"),
                   trControl = trControl)

# Logistic regression model
set.seed(123)
logmod <- train(
  frmla, 
  Data, 
  method = "glmnet",                 
  family = 'multinom',
  trControl = trControl)

# KNN model
set.seed(123)
KNN.model <- train(
  frmla,
  Data2,
  method = "knn",
  preProc = c("center", "scale"), 
  trControl = trControl,
  tuneGrid = expand.grid(k = seq(1, 15, 1)))

# C5.0 Tree 
set.seed(123)
C50_Mod <- train(frmla, 
                 Data,
                 method = "C5.0",
                 trControl = trControl,
                 preProc = c("center", "scale"), 
                 metric = "Accuracy",
                 importance = 'impurity')

# Random Forest
set.seed(123)
RF.model <- train(frmla, 
                  Data,
                  method = "ranger",
                  trControl = trControl,
                  preProc = c("center", "scale"), 
                  metric = "Accuracy",
                  importance = 'impurity')

# GBM model 
set.seed(123)
GBM.model <- train(frmla, 
                   Data,
                   method = "gbm",
                   preProc = c("center", "scale"), 
                   trControl = trControl,
                   metric = "Accuracy")

# SVMs
set.seed(123)
SVML.model <- train(frmla, 
                    Data,
                    method = "svmLinear",
                    trControl = trControl,
                    preProc =  c('center', 'scale'),
                    metric = "Accuracy",
                    importance = 'impurity')

set.seed(123)
SVMR.model <- train(frmla, 
                    Data,
                    method = "svmRadial",
                    preProc =  c('center', 'scale'),
                    trControl = trControl,
                    metric = "Accuracy",
                    importance = 'impurity')

set.seed(123)
SVMP.model <- train(frmla, 
                    Data,
                    method = "svmPoly",
                    preProc =  c('center', 'scale'),
                    trControl = trControl,
                    metric = "Accuracy",
                    importance = 'impurity')

# Naïve Bayes
set.seed(123)
NB.model <- train(frmla, 
                  Data,
                  preProc = c("center", "scale"), 
                  method = "nb",
                  trControl = trControl,
                  metric = "Accuracy")
```


## **3. Results**   

### **3.1 Texture metrics**   

A general MANOVA considering all groups and variables shows marked statistically significant differences between groups.    

```{r}
# MANOVA on data
res.man <- manova(cbind(Mean, Median, Modal, SD, Kurtosis, Skewness, 
                        Rq, Ra, Rsk, Rku,
                        ASM, CONT, CORR, IDM, ENT) ~ Flake.Time, 
                  data = Sequential.Data)
summary(res.man)
```

MANOVA analysis comparing a category with its subsequent time of exposure shows marked statistical differences between fresh materials and one hour of rounding 
```{r}

# Set different groups
x <- Sequential.Data %>% filter(Flake.Time == "Fresh" | Flake.Time == "One.Hour")
y <- Sequential.Data %>% filter(Flake.Time == "Five.Hours" | Flake.Time == "One.Hour")
z <- Sequential.Data %>% filter(Flake.Time == "Five.Hours" | Flake.Time == "Ten.Hours")
t <- Sequential.Data %>% filter(Flake.Time == "Neocortex" | Flake.Time == "Ten.Hours")

# MANOVA fresh vs one hour
res.man <- manova(cbind(Mean, Median, Modal, SD, Kurtosis, Skewness, 
                        Rq, Ra, Rsk, Rku,
                        ASM, CONT, CORR, IDM, ENT) ~ Flake.Time, data = x)
summary(res.man)

# MANOVA one hour vs five hours
res.man <- manova(cbind(Mean, Median, Modal, SD, Kurtosis, Skewness, 
                        Rq, Ra, Rsk, Rku,
                        ASM, CONT, CORR, IDM, ENT) ~ Flake.Time, data = y)
summary(res.man)

# MANOVA five hours vs Ten hours
res.man <- manova(cbind(Mean, Median, Modal, SD, Kurtosis, Skewness, 
                        Rq, Ra, Rsk, Rku,
                        ASM, CONT, CORR, IDM, ENT) ~ Flake.Time, data = z)
summary(res.man)

# MANOVA Ten hours vs Neocortex
res.man <- manova(cbind(Mean, Median, Modal, SD, Kurtosis, Skewness, 
                        Rq, Ra, Rsk, Rku,
                        ASM, CONT, CORR, IDM, ENT) ~ Flake.Time, data = t)
summary(res.man)
```

When considering all variables, no statistical differences were found between images exposed to five and ten hours rounding. However, statistical difference between these two categories are present in the mean, median, standard deviation, kurtosis, skewness, Rq, Ra, Rku, ASM, contrast, correlation, IDM and entropy.    

```{r}
# MANOVA five hours vs Ten hours
res.man <- manova(cbind(Mean, Median, Modal, SD, Kurtosis, Skewness, 
                        Rq, Ra, Rsk, Rku,
                        ASM, CONT, CORR, IDM, ENT) ~ Flake.Time, data = z)
summary(res.man)
summary.aov(res.man)
```

Exploratory visual analysis shows a series of clear tendencies for the three groups of statistics employed. On general terms, as sedimentary abrasion increases, images will present increasing values of mean, median, standard deviation, Ra, Rq, contrast (CONT) and entropy (ENT). By the contrary, as sedimentary abrasion increases, images will present decreasing values of kurtosis, skewness, Rku, Rsk, angular second moment (ASM), correlation between pixels (CORR) and inverse different moment (IDM). Measures of central tendency (mean, median and modal) are the less reliable, since the three times of cumulative abrasion show important overlapping values for these statistic variables. Effects of sedimentary abrasion are especially observable in variables of data dispersion (kurtosis, skewness and standard deviation), and in the five textural features (angular second moment: ASM; contrast: CONT; correlation: CORR; entropy: ENT and inverse different moment: IDM). As sedimentary abrasion increases, images will become less homogeneous, with increasing amounts of local changes which can be related with an increasing roughness.    

```{r, fig.height=17, fig.width=10}
Sequential.Data  %>%
    pivot_longer(
    Mean:ENT,
    names_to = "Variables",
    values_to = "values") %>% 
  mutate(Flake.Time = factor(Flake.Time, labels = c("Fresh", "1h", "5h",
                                                    "10h", "Neocort.")),
         Variables = factor(Variables, levels = c("Mean", "Median", "Modal", "SD",
                                                      "Kurtosis", "Skewness",
                                                    "Ra", "Rq", "Rku", "Rsk",
                                                    "ASM", "CONT", "CORR", "ENT",
                                                    "IDM"
                                                      ))) %>% 
  
  ggplot(aes(Flake.Time, values, fill = Flake.Time)) +
  geom_violin(alpha = 0.4) +
  geom_boxplot(outlier.shape = NA,  width = 0.4) +
  geom_jitter(width = 0.1, shape = 23, size = 0.75, aes(fill = Flake.Time)) +
  scale_fill_brewer(palette = "Oranges") +
  ylab("Metrics") +
  xlab("Time under sedimentary abrassion") +
  facet_wrap(~ Variables, scales = "free", ncol = 3) +
  theme_light() +
  theme(
    legend.position = "none",
    strip.text = element_text(color = "black", face = "bold", size = 8),
    strip.background = element_rect(fill = "white", colour = "black", linewidth = 1),
    axis.title = element_text(color = "black", size = 8),
    axis.text.x = element_text(color = "black", size = 6.5),
    axis.text.y = element_text(color = "black", size = 7)
  )
```

Although trends in surface change by sedimentary abrasion are clear, the exploratory visual analysis also indicates important overlapping between exposure time categories. A visual evaluation of images shows that, within lithic artifacts, the development of sedimentary abrasion is heterogeneous. Convex surfaces and areas close to the ridges and edges developed abrasion more quickly and intensely than other areas.    

![Examples of differential abrasion among the same artifact. Top: images of the fresh surface. Bottom: sequential images of sedimentary abrasion: a) little or no abrasion is developed; b) sedimentary abrasion is moderately developed; c) sedimentary abrasion is heavily developed.](Figures/07-Differential-development.png){heigh=300}
![Examples of differential abrasion among the same artifact. Top: images of the fresh surface. Bottom: sequential images of sedimentary abrasion: a) sedimentary abrasion develops moderately/strongly; b) sedimentary abrasion is lightly developed; c) sedimentary abrasion is strongly developed.](Figures/08-Differential-development2.png){heigh=300}

### **3.2 Machine Learning models results**    

presents the performance metrics (general precision and AUC) for each of the tested machine learning models after the 100x10 fold cross validation. Although all models presented general precision values below 0.5, these values were in all cases substantially higher than the “no-information rate” (0.296). Of the ten tested models, the logistic regression presented the highest general precision value (0.485), with the linear discriminant analysis (LDA) presenting a very similar value (0.479). Random Forest and decision tree with C5.0 presented the lowest general precision with respective values of 0.381 and 0.402. Use of the ROC curves for model evaluation indicates that all models presented good or acceptable general AUC values with the exception of the SVM with polynomial kernel which presented a poor general AUC (0.68). The linear discriminant analysis (LDA) presented the highest AUC value (0.83), and substantially higher than the logistic regression (0.819). Thus, it can be considered that the LDA model performed the best when differentiating degree of sedimentary abrasion.    

```{r, include=FALSE}
load("Data/AI-models-Surface.RData")
```
```{r}
# Get Precision and AUC of each model
data.frame(
  "Model" = c("LDA", "KNN", "Log. Reg.", "SVML", "SVMP", "SVMR",
              "C5.0", "Rand. Forest", "GBM", "Naïve Bayes"),
  "Accuracy" =
    rbind(
      round(confusionMatrix(LDA.model$pred$pred, LDA.model$pred$obs)[[3]][c(1)],3),
      round(confusionMatrix(KNN.model$pred$pred, KNN.model$pred$obs)[[3]][c(1)],3),
      round(confusionMatrix(logmod$pred$pred, logmod$pred$obs)[[3]][c(1)],3),
      round(confusionMatrix(SVML.model$pred$pred, SVML.model$pred$obs)[[3]][c(1)],3),
      round(confusionMatrix(SVMP.model$pred$pred, SVMP.model$pred$obs)[[3]][c(1)],3),
      round(confusionMatrix(SVMR.model$pred$pred, SVMR.model$pred$obs)[[3]][c(1)],3),
      round(confusionMatrix(C50_Mod$pred$pred, C50_Mod$pred$obs)[[3]][c(1)],3),
      round(confusionMatrix(RF.model$pred$pred, RF.model$pred$obs)[[3]][c(1)],3),
      round(confusionMatrix(GBM.model$pred$pred, GBM.model$pred$obs)[[3]][c(1)],3),
      round(confusionMatrix(NB.model$pred$obs, NB.model$pred$pred)[[3]][c(1)],3)),
  "AUC" =
    rbind(
      round(pROC::multiclass.roc(LDA.model$pred$obs, LDA.model$pred[,4:8])$auc[[1]],3),
      round(pROC::multiclass.roc(KNN.model$pred$obs, KNN.model$pred[,4:8])$auc[[1]], 3),
      round(pROC::multiclass.roc(logmod$pred$obs, logmod$pred[,6:10])$auc[[1]], 3),
      round(pROC::multiclass.roc(SVML.model$pred$obs, SVML.model$pred[,4:8])$auc[[1]], 3),
      round(pROC::multiclass.roc(SVMP.model$pred$obs, SVMP.model$pred[,6:10])$auc[[1]], 3),
      round(pROC::multiclass.roc(SVMR.model$pred$obs, SVMR.model$pred[,5:9])$auc[[1]], 3),
      round(pROC::multiclass.roc(C50_Mod$pred$obs, C50_Mod$pred[,7:11])$auc[[1]], 3),
      round(pROC::multiclass.roc(RF.model$pred$obs, RF.model$pred[,6:10])$auc[[1]], 3),
      round(pROC::multiclass.roc(GBM.model$pred$obs, GBM.model$pred[,8:12])$auc[[1]], 3),
      round(pROC::multiclass.roc(NB.model$pred$obs, NB.model$pred[,6:10])$auc[[1]], 3))) %>% 
  mutate(Model = factor(Model, levels = c("LDA", "KNN", "Log. Reg.", "SVML", "SVMP", "SVMR",
                                          "C5.0", "Rand. Forest", "GBM", "Naïve Bayes"))) %>% 
  
  pivot_longer(
    cols = c(Accuracy, AUC),
    values_to = "values",
    names_to = "Variables") %>% 
  
  ggplot(aes(values, Model, fill = Model)) +
  geom_col() +
  facet_wrap(~ Variables, scales = "free") +
  ggsci::scale_fill_npg() +
  geom_text(aes(label = (values)), hjust = "top", size = 2.5) +
  theme_light() +
  xlab("Values") +
  theme(
    legend.position = "none",
    strip.text = element_text(color = "black", face = "bold", size = 11),
    strip.background = element_rect(fill = "white", colour = "black", linewidth = 0.75),
    axis.text = element_text(color = "black")
  )
```

The following code presents the ROC curves and AUC’s obtained for each category of sedimentary abrasion using the LDA model. Individual AUC values of fresh surfaces and neocortex were outstanding (respective values of 0.9 and 0.98). Individual AUC values of the different times of exposure varied from being fair in the case of one (0.71) and ten (0.74) hours of exposure, to being poor in the case of five hours (0.62) of exposure. 

```{r}
#### Confusion matrix ####
# Obtain from caret and reshape
Cnf.Matr <- confusionMatrix(LDA.model)$table
Cnf.Matr <- reshape2::melt(Cnf.Matr)

# Normalize the data
Cnf.Matr %>% mutate(
  value = case_when(
    Reference == "Fresh" ~ (value/sum(confusionMatrix(LDA.model)$table[1:5]))*100,
    Reference == "One.Hour" ~ (value/sum(confusionMatrix(LDA.model)$table[6:10]))*100,
    Reference == "Five.Hours" ~ (value/sum(confusionMatrix(LDA.model)$table[11:15]))*100,
    Reference == "Ten.Hours" ~ (value/sum(confusionMatrix(LDA.model)$table[16:20]))*100,
    Reference == "Neocortex" ~ (value/sum(confusionMatrix(LDA.model)$table[21:25]))*100,
  )) %>% 
  mutate(
    Prediction = factor(Prediction, 
                        levels = c("Neocortex", "Ten.Hours", "Five.Hours",
                                   "One.Hour", "Fresh"),
                        labels = c("Neocort.", "10 h", "5 h", "1 h", "Fresh")),
    Reference = factor(Reference,
                       levels = c("Fresh", "One.Hour", "Five.Hours", 
                                  "Ten.Hours", "Neocortex"),
                       labels = c("Fresh", "1 h", "5 h", 
                                  "10 h", "Neocort."))) %>% 
  
  # And plot cnfusion matrix
  ggplot(aes(Reference, Prediction, fill = value)) + 
  geom_tile(alpha = 0.75) +
  geom_text(aes(label = round(value, 2)), size = 2.5) +
  scale_fill_gradient(low = "white", high = "blue")  +
  scale_x_discrete(position = "top") +
  theme_bw() +
  coord_fixed() +
  theme(legend.position = "none",
        axis.title = element_text(size = 8, color = "black", face = "bold"),
        axis.text = element_text(size = 7.5, color = "black"),
        title = element_text(size = 8, color = "black", face = "bold"))
```
```{r}
#### Final ROC curve and AUC ####
MC.ROC <- LDA.model$pred %>% 
  select(pred, obs, Fresh, One.Hour, Five.Hours, Ten.Hours, Neocortex) %>% 
  mutate(
    FreshvsAll = case_when(
      obs == "Neocortex" ~ "Rest",
      obs == "Ten.Hours" ~ "Rest", 
      obs == "Five.Hours" ~ "Rest", 
      obs == "One.Hour" ~ "Rest", 
      obs == "Fresh" ~ "Fresh"),
    
    OnehvsAll = case_when(
      obs == "Neocortex" ~ "Rest",
      obs == "Ten.Hours" ~ "Rest", 
      obs == "Five.Hours" ~ "Rest", 
      obs == "One.Hour" ~ "One.Hour", 
      obs == "Fresh" ~ "Rest"),
    
    FivevsAll = case_when(
      obs == "Neocortex" ~ "Rest",
      obs == "Ten.Hours" ~ "Rest", 
      obs == "Five.Hours" ~ "Five.Hours", 
      obs == "One.Hour" ~ "Rest", 
      obs == "Fresh" ~ "Rest"),
    
    TenvsAll = case_when(
      obs == "Neocortex" ~ "Rest",
      obs == "Ten.Hours" ~ "Ten.Hours", 
      obs == "Five.Hours" ~ "Rest", 
      obs == "One.Hour" ~ "Rest", 
      obs == "Fresh" ~ "Rest"),
    
    NeovsAll = case_when(
      obs == "Neocortex" ~ "Neocortex",
      obs == "Ten.Hours" ~ "Rest", 
      obs == "Five.Hours" ~ "Rest", 
      obs == "One.Hour" ~ "Rest", 
      obs == "Fresh" ~ "Rest"))

library(pROC)
x <- roc(MC.ROC$FreshvsAll, MC.ROC$Fresh)
RF.ROCs <- data.frame(
  Sensi = x$sensitivities,
  Speci = x$specificities,
  Class = "Fresh")

x <- roc(MC.ROC$OnehvsAll, MC.ROC$One.Hour)
temp <- data.frame(
  Sensi = x$sensitivities,
  Speci = x$specificities,
  Class = "One.Hour")
RF.ROCs <- rbind(RF.ROCs, temp)

x <- roc(MC.ROC$FivevsAll, MC.ROC$Five.Hours)
temp <- data.frame(
  Sensi = x$sensitivities,
  Speci = x$specificities,
  Class = "Five.Hours")
RF.ROCs <- rbind(RF.ROCs, temp)

x <- roc(MC.ROC$TenvsAll, MC.ROC$Ten.Hours)
temp <- data.frame(
  Sensi = x$sensitivities,
  Speci = x$specificities,
  Class = "Ten.Hours")
RF.ROCs <- rbind(RF.ROCs, temp)

x <- roc(MC.ROC$NeovsAll, MC.ROC$Neocortex)
temp <- data.frame(
  Sensi = x$sensitivities,
  Speci = x$specificities,
  Class = "Neocortex")
RF.ROCs <- rbind(RF.ROCs, temp)

#Set factors (otherwise legend will not correspond)
RF.ROCs$Class <- factor(RF.ROCs$Class, 
                        levels = c(
                          "Neocortex",
                          "Ten.Hours",
                          "Five.Hours", 
                          "One.Hour",
                          "Fresh" 
                        ))

# Plot the three ROC's and AUC's in legend
RF.ROCs %>% 
  ggplot(aes(Speci, Sensi, color = Class)) +
  geom_line(linewidth = 1.01) +
  scale_x_continuous(trans = "reverse") +
  ggsci::scale_color_aaas(
    labels = c(paste0("Neocortex ", "(AUC = ", round(auc(MC.ROC$NeovsAll, MC.ROC$Neocortex)[[1]],2), ")"),
               paste0("Ten Hours ", "(AUC = ", round(auc(MC.ROC$TenvsAll, MC.ROC$Ten.Hours)[[1]],2), ")"),
               paste0("Five Hours ", "(AUC = ", round(auc(MC.ROC$FivevsAll, MC.ROC$Five.Hours)[[1]], 2), ")"),
               paste0("One Hour ", "(AUC = ", round(auc(MC.ROC$OnehvsAll, MC.ROC$One.Hour)[[1]], 2), ")"),
               paste0("Fresh ", "(AUC = ", round(auc(MC.ROC$FreshvsAll, MC.ROC$Fresh)[[1]], 2), ")"))) +
  coord_fixed() +
  theme_light() +
  xlab("Specificities") +
  ylab("Sensitivities") +
  geom_abline(intercept = 1, slope = 1) +
  theme(
    legend.title = element_text(color = "black", face = "bold", size = 9),
    legend.text = element_text(color = "black", size = 8),  
    axis.text = element_text(color = "black", size = 10),
    axis.title = element_text(color = "black", size = 11, face = "bold"))
```

The confusion matrix provides insights into the sources and directionality of the confusions. As time of exposure increased, the number of photographs being identified as fresh diminished, with a minimum portion (1.66) being identified as fresh after ten hours of rounding. In the same way, after ten hours of rounding a small portion of photographs were starting to resemble neocortex (11.91). The confusion matrix reinforces previous observations through exploratory visual analysis and direct examination of photographs that sedimentary abrasion is not being developed in a homogeneous way, with some surfaces from the same flake developing abrasion faster than others. Despite this, a clear directionality (as sedimentary abrasion increases, the number of photographs identified as fresh diminishes) is observed. Additional training of an LDA model on values from the first five PC (99% of the variance) showed little changes in general precision (0.464) or general AUC (0.817) regarding the LDA model trained with all non-perfectly collinear variables.   

### **3.3. Feature importance**    

The following figure presents feature importance according to exposure time to time of rounding and average importance.    

```{r}
#### Extract variable importance from LDA model ####
Var.Importance <- data.frame(varImp(LDA.model)$importance) 

Var.Importance %>% 
  mutate(Mean.Importance = rowMeans(Var.Importance),
         Feature = rownames(Var.Importance)) %>% 
  arrange(-Mean.Importance) %>% 
  
  pivot_longer(
    Fresh:Mean.Importance,
    names_to = "Time",
    values_to = "Values") %>% 
  
  mutate(
    Time = factor(Time, 
                  levels = c(
                    "Fresh", "One.Hour","Five.Hours", 
                    "Ten.Hours", "Neocortex", "Mean.Importance"),
                  labels = c(
                    "Fresh", "1h","5h", "10h", "Neocort.",
                    "Mean Importance"))) %>% 
  
  ggplot(aes(Values, reorder(Feature, Values), fill = Values)) +
  facet_wrap(~ Time, scales = "free", ncol = 2) +
  geom_bar(stat= "identity", position = "dodge") +
  geom_text(aes(label = round(Values, 2)), 
            position = position_stack(vjust = 0.5), size = 2) +
  scale_fill_gradient(low = "red", high = "blue") +
  guides(fill = "none") +
  ylab(NULL) +
  xlab("Mean importance") +
  theme_light() +
  theme(
    strip.text = element_text(color = "black", face = "bold", size = 8),
    strip.background = element_rect(fill = "white", colour = "black", linewidth = 1),
    axis.text.y = element_text(color = "black", size = 6),
    axis.text.x = element_text(color = "black", size = 7),
    axis.title.x = element_text(color = "black", size = 9),
    axis.title.y = element_text(color = "black", size = 9))
```

On general terms the LDA does not favor any group of statistics over other. The three most important features considered by the LDA model relate to the distribution of values (kurtosis), texture (contrast) and roughness (Rq). Although the rest of variables present similar values of importance, it is important to consider the high levels of collinearity previously observed between features which are probably affecting the interpretation of the rest of the features. Entropy (ENT) and angular second moment (ASM) presented clear trends in the visual exploratory analysis. However, the LDA model considers them of less important, probably due to intense overlapping of values from different exposure time.   

```{r}
ggpubr::ggarrange(
  (Sequential.Data %>% ggplot(aes(Kurtosis, ENT, color = Flake.Time)) +
     geom_point(alpha = 0.5) +
     stat_ellipse(aes(color = Flake.Time)) +
     scale_color_manual(
       name = "Expsoure time",
       values = c("blue", "royalblue4", "darkorange1", "red", "darkred"),
       labels = c("Fresh", "1h","5h", "10h", "Neocort.")) +
     theme_light() +
     theme(
       axis.text = element_text(size = 9, color = "black"),
       axis.title = element_text(size = 9.25, color = "black", face = "bold"))
  ),
  
  (
    Sequential.Data %>% ggplot(aes(CONT, ASM, color = Flake.Time)) +
      geom_point(alpha = 0.5) +
      stat_ellipse(aes(color = Flake.Time)) +
      scale_color_manual(
        name = "Expsoure time",
        values = c("blue", "royalblue4", "darkorange1", "red", "darkred"),
        labels = c("Fresh", "1h","5h", "10h", "Neocort.")) +
      theme_light() +
      theme(
        axis.text = element_text(size = 9, color = "black"),
        axis.title = element_text(size = 9.25, color = "black", face = "bold"))
  ),
  ncol = 2,
  legend = "bottom",
  common.legend = TRUE
)
```


## References

</div>   